{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "Here is my solution to the third project of the udacity deep reinforcement learning nanodegree.\n",
    "\n",
    "The goal of this project is training agents to play tennis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from agent import Agent\n",
    "from replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells load the tennis environment and describe the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent and Neural Network architecture\n",
    "Here is the initialization of the two ddpg agent and all the hyper parameters used (minus the model's layer unit sizes which I have defined in model.py)\n",
    "\n",
    "I kept the model, agent and hyperparameters very close to what I used in project 2. I had to tweak a few things. \n",
    "\n",
    "Most importantly the two agents share a replay buffer such that their experience is combined when training. \n",
    "\n",
    "I  increased the value of the batch size which seemed to help by making the agent accumulate more experiences before the actor and critic start to learn. (The agent don't start learning until after the replay buffer has more than batch_size examples)\n",
    "\n",
    "The model for the actor has three fully connected layers with a rectified linear unit (ReLU) activation function on the first two layers and uses tanh after the third layer. First two layers are 400, 300 units in size. The first 2 layers use relu as activation functions and the final layer uses tanh. \n",
    "\n",
    "The critic network uses 3 fully connected layers. This is down from 4 for the previous project. This seemed to help improve the speed of learning the value function. Layer 1 has 256 units, layer 2 has 128 units. Layer 1 and 2 use relu activation functions. \n",
    "\n",
    "Noise was added using an Ornstein-Uhlenbeck process (as recommended in the paper) theta and sigma were set as the same values as the paper 0.15 and 0.2 respectively. I noticed that the agents struggled to learn after a certain amount of episodes and tried removing the noise. This improved learning significantly. \n",
    "\n",
    "Another thing I changed was that I set weight_decay to 0. With weight_decay the agents seemed to get stuck either at the edge of their play area. After I removed the weight_decay accidentally on one of the agent I noticed that it was starting to succeeded at tracking the ball while the other was not. I removed it from both and finally was able to solve the problem. I may have had the values for weight decay set incorrectly. \n",
    "\n",
    "\n",
    "Here is the initalization of the agent and all the hyper parameters used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE=int(1e6)\n",
    "BATCH_SIZE=512\n",
    "GAMMA=0.99     \n",
    "TAU=1e-2\n",
    "LR_ACTOR=1e-3     \n",
    "LR_CRITIC=1e-3      \n",
    "WEIGHT_DECAY=0\n",
    "RANDOM_SEED=0\n",
    "\n",
    "agent1 = Agent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,    \n",
    "    gamma=GAMMA,           \n",
    "    tau=TAU,\n",
    "    lr_actor=LR_ACTOR,       \n",
    "    lr_critic=LR_CRITIC,        \n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    random_seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "agent2 = Agent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    memory=agent1.memory, # Sharing replay memory between ddpg agents. \n",
    "    batch_size=BATCH_SIZE,    \n",
    "    gamma=GAMMA,           \n",
    "    tau=TAU,\n",
    "    lr_actor=LR_ACTOR,       \n",
    "    lr_critic=LR_CRITIC,        \n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    random_seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.00 best_score 0.09000000171363354\n",
      "Episode 200\tAverage Score: 0.00 best_score 0.09000000171363354\n",
      "Episode 300\tAverage Score: 0.04 best_score 0.10000000149011612\n",
      "Episode 400\tAverage Score: 0.13 best_score 0.90000001341104516\n",
      "Episode 462\tAverage Score: 0.50 best_score 2.6000000387430191\n",
      "Environment solved in 462 episodes!\tAverage Score: 0.50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "STOP_NOISE_AFTER_EP=300\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "def ddpg(n_episodes=1500, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    best_score = 0.0\n",
    "    add_noise = True\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        if i_episode > STOP_NOISE_AFTER_EP:\n",
    "            add_noise = False\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        agent1.reset()\n",
    "        agent2.reset()\n",
    "        scores_ep = np.zeros(num_agents)\n",
    "        while True:\n",
    "            action1 = agent1.act(states[0], add_noise=add_noise).tolist()\n",
    "            action2 = agent2.act(states[1], add_noise=add_noise).tolist()\n",
    "            actions = [action1, action2]\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            agent1.step(states[0], action1, rewards[0], next_states[0], dones[0])\n",
    "            agent2.step(states[1], action2, rewards[1], next_states[1], dones[1])\n",
    "            scores_ep += rewards\n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        max_score = np.max(scores_ep)\n",
    "        scores_deque.append(max_score)\n",
    "        scores.append(max_score)\n",
    "        if max_score > best_score:\n",
    "            best_score = max_score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} best_score {}'.format(i_episode, np.mean(scores_deque), best_score), end=\"\")\n",
    "        torch.save(agent1.actor_local.state_dict(), 'checkpoint_actor1.pth')\n",
    "        torch.save(agent1.critic_local.state_dict(), 'checkpoint_critic1.pth')\n",
    "        torch.save(agent2.actor_local.state_dict(), 'checkpoint_actor2.pth')\n",
    "        torch.save(agent2.critic_local.state_dict(), 'checkpoint_critic2.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            torch.save(agent1.actor_local.state_dict(), 'checkpoint_actor1.pth')\n",
    "            torch.save(agent1.critic_local.state_dict(), 'checkpoint_critic1.pth')\n",
    "            torch.save(agent2.actor_local.state_dict(), 'checkpoint_actor2.pth')\n",
    "            torch.save(agent2.critic_local.state_dict(), 'checkpoint_critic2.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.10000000149011612\n",
      "Score (max over agents) from episode 2: 0.7000000104308128\n",
      "Score (max over agents) from episode 3: 0.4000000059604645\n",
      "Score (max over agents) from episode 4: 0.6000000089406967\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        action1 = agent1.act(states[0], add_noise=False).tolist()\n",
    "        action2 = agent2.act(states[1], add_noise=False).tolist()\n",
    "        actions = [action1, action2]\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "I've seen suggestions that others have had success with similar problems using using a Proximal Policy Optimization algorithm instead ddpg.\n",
    "\n",
    "Another thing I'd like to train is share more information between agents. For example having the actor networks sync weights (I was thinking throught averaging them) after a certain amount of episodes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
